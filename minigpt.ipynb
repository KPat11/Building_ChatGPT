{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Finding unique characters for encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('homer.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of dataset:  908201\n"
     ]
    }
   ],
   "source": [
    "print(\"length of dataset: \", len(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Project Gutenberg eBook of The Iliad\n",
      "    \n",
      "This ebook is for the use of anyone anywhere in the United States and\n",
      "most other parts of the world at no cost and with almost no restrictions\n",
      "whatsoever. You may copy it, give it away or re-use it under the terms\n",
      "of the Project Gutenberg License included with this ebook or online\n",
      "at www.gutenberg.org. If you are not located in the United States,\n",
      "you will have to check the laws of the country where you are located\n",
      "before using this eBook.\n",
      "\n",
      "Title: The Iliad\n",
      "\n",
      "Author: Homer\n",
      "\n",
      "Translator: Samuel Butler\n",
      "\n",
      "Release date: June 1, 2000 [eBook #2199]\n",
      "                Most recently updated: August 16, 2022\n",
      "\n",
      "Language: English\n",
      "\n",
      "Credits: Jim TinsleyRevised by Richard Tonsing.\n",
      "\n",
      "\n",
      "*** START OF THE PROJECT GUTENBERG EBOOK THE ILIAD ***\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "      THE ILIAD OF HOMER\n",
      "\n",
      "      Rendered into English Prose for\n",
      "      the use of those who cannot\n",
      "      read the original\n",
      "\n",
      "\n",
      "      by Samuel Butler\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Contents\n",
      "\n",
      "\n",
      " BOOK I.\n",
      " BOOK II.\n",
      " BOOK III.\n",
      " BOOK IV.\n",
      " BOOK V.\n",
      " BOOK VI.\n",
      " BOO\n"
     ]
    }
   ],
   "source": [
    "#Looking at first 1000 characters\n",
    "print(text[:1001])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " !#$%()*,-./0123456789:;?ABCDEFGHIJKLMNOPQRSTUVWXYZ[]abcdefghijklmnopqrstuvwxyz—‘’“”•™﻿\n",
      "88\n"
     ]
    }
   ],
   "source": [
    "# unique characters that occur in text\n",
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "print(''.join(chars))\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Tokenization: Encoding and Decoding Strategy\n",
    "\n",
    "I will be mapping characters to numbers and create functions to encode and decode. I know their are other methods like Sentencepiece or a byte-pair tokenizer like tiktoken which openai uses but I elected to code out instead of using libraries for learning/practice purposes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[61, 58, 65, 65, 68, 68, 68, 1, 59, 71, 62, 58, 67, 57]\n",
      "hellooo friend\n"
     ]
    }
   ],
   "source": [
    "# Mapping\n",
    "encode_map = { ch:i for i,ch in enumerate(chars) }\n",
    "decode_map = { i:ch for i, ch in enumerate(chars) }\n",
    "\n",
    "#encoder takes string and maps to list of integers\n",
    "encode = lambda e: [encode_map[c] for c in e]\n",
    "#decode takes list of integers and outputs a string\n",
    "decode = lambda d: ''.join([decode_map[u] for u in d])\n",
    "\n",
    "print(encode(\"hellooo friend\"))\n",
    "print(decode(encode(\"hellooo friend\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torch'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# encoding entire dataset using pytorch\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\n\u001b[32m      3\u001b[39m data = torch.tensor(encode(text), dtype=torch.long)\n\u001b[32m      4\u001b[39m \u001b[38;5;28mprint\u001b[39m(data.shape, data.dtype)\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'torch'"
     ]
    }
   ],
   "source": [
    "# encoding entire dataset using pytorch\n",
    "import torch\n",
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "print(data.shape, data.dtype)\n",
    "print(data[:1001]) #peek at first 1000 characters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
