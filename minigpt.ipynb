{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Finding unique characters for encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('homer.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of dataset:  908201\n"
     ]
    }
   ],
   "source": [
    "print(\"length of dataset: \", len(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Project Gutenberg eBook of The Iliad\n",
      "    \n",
      "This ebook is for the use of anyone anywhere in the United States and\n",
      "most other parts of the world at no cost and with almost no restrictions\n",
      "whatsoever. You may copy it, give it away or re-use it under the terms\n",
      "of the Project Gutenberg License included with this ebook or online\n",
      "at www.gutenberg.org. If you are not located in the United States,\n",
      "you will have to check the laws of the country where you are located\n",
      "before using this eBook.\n",
      "\n",
      "Title: The Iliad\n",
      "\n",
      "Author: Homer\n",
      "\n",
      "Translator: Samuel Butler\n",
      "\n",
      "Release date: June 1, 2000 [eBook #2199]\n",
      "                Most recently updated: August 16, 2022\n",
      "\n",
      "Language: English\n",
      "\n",
      "Credits: Jim TinsleyRevised by Richard Tonsing.\n",
      "\n",
      "\n",
      "*** START OF THE PROJECT GUTENBERG EBOOK THE ILIAD ***\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "      THE ILIAD OF HOMER\n",
      "\n",
      "      Rendered into English Prose for\n",
      "      the use of those who cannot\n",
      "      read the original\n",
      "\n",
      "\n",
      "      by Samuel Butler\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Contents\n",
      "\n",
      "\n",
      " BOOK I.\n",
      " BOOK II.\n",
      " BOOK III.\n",
      " BOOK IV.\n",
      " BOOK V.\n",
      " BOOK VI.\n",
      " BOO\n"
     ]
    }
   ],
   "source": [
    "#Looking at first 1000 characters\n",
    "print(text[:1001])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " !#$%()*,-./0123456789:;?ABCDEFGHIJKLMNOPQRSTUVWXYZ[]abcdefghijklmnopqrstuvwxyz—‘’“”•™﻿\n",
      "88\n"
     ]
    }
   ],
   "source": [
    "# unique characters that occur in text\n",
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "print(''.join(chars))\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Tokenization: Encoding and Decoding Strategy\n",
    "\n",
    "I will be mapping characters to numbers and create functions to encode and decode. I know their are other methods like Sentencepiece or a byte-pair tokenizer like tiktoken which openai uses but I elected to code out instead of using libraries for learning/practice purposes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[61, 58, 65, 65, 68, 68, 68, 1, 59, 71, 62, 58, 67, 57]\n",
      "hellooo friend\n"
     ]
    }
   ],
   "source": [
    "# Mapping\n",
    "encode_map = { ch:i for i,ch in enumerate(chars) }\n",
    "decode_map = { i:ch for i, ch in enumerate(chars) }\n",
    "\n",
    "#encoder takes string and maps to list of integers\n",
    "encode = lambda e: [encode_map[c] for c in e]\n",
    "#decode takes list of integers and outputs a string\n",
    "decode = lambda d: ''.join([decode_map[u] for u in d])\n",
    "\n",
    "print(encode(\"hellooo friend\"))\n",
    "print(decode(encode(\"hellooo friend\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([908201]) torch.int64\n",
      "tensor([87, 45, 61,  ..., 27, 40, 40])\n"
     ]
    }
   ],
   "source": [
    "# encoding entire dataset using pytorch\n",
    "import torch\n",
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "print(data.shape, data.dtype)\n",
    "print(data[:1001]) #peek at first 1000 characters\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Split into Train/Test\n",
    "Splitting train/test, chunk definitions, and batching for multiple chunks at same time.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([87, 45, 61,  ..., 56, 61, 58])\n",
      "tensor([72, 73, 67,  ...,  0,  0,  0])\n"
     ]
    }
   ],
   "source": [
    "#taking 90% of data for train, and rest for validation\n",
    "n = int(0.9*len(data))\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]\n",
    "print(train_data)\n",
    "print(val_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([87, 45, 61, 58,  1, 41, 71, 68, 63])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# will be training random chunks rather than every line for computation reasons\n",
    "chunk_size = 8\n",
    "train_data[:chunk_size+1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "When input is tensor([87]) the target is: 45\n",
      "When input is tensor([87, 45]) the target is: 61\n",
      "When input is tensor([87, 45, 61]) the target is: 58\n",
      "When input is tensor([87, 45, 61, 58]) the target is: 1\n",
      "When input is tensor([87, 45, 61, 58,  1]) the target is: 41\n",
      "When input is tensor([87, 45, 61, 58,  1, 41]) the target is: 71\n",
      "When input is tensor([87, 45, 61, 58,  1, 41, 71]) the target is: 68\n",
      "When input is tensor([87, 45, 61, 58,  1, 41, 71, 68]) the target is: 63\n"
     ]
    }
   ],
   "source": [
    "# Setting up next likely value logic and sanity checking\n",
    "\n",
    "x = train_data[:chunk_size]\n",
    "y = train_data[1:chunk_size+1]\n",
    "for i in range(chunk_size):\n",
    "    context = x[:i+1]\n",
    "    target = y[i]\n",
    "    print(f\"When input is {context} the target is: {target}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs:\n",
      "torch.Size([4, 8])\n",
      "tensor([[67,  1, 54,  1, 59, 62, 67, 58],\n",
      "        [73, 68, 71,  1, 56, 54, 65, 65],\n",
      "        [58, 71, 58,  1, 54,  1, 59, 54],\n",
      "        [75, 58,  1, 61, 54, 73, 58, 72]])\n",
      "targets:\n",
      "torch.Size([4, 8])\n",
      "tensor([[ 1, 54,  1, 59, 62, 67, 58,  1],\n",
      "        [68, 71,  1, 56, 54, 65, 65, 58],\n",
      "        [71, 58,  1, 54,  1, 59, 54, 65],\n",
      "        [58,  1, 61, 54, 73, 58, 72,  1]])\n",
      "----\n",
      "When input is [67] the target is: 1\n",
      "When input is [67, 1] the target is: 54\n",
      "When input is [67, 1, 54] the target is: 1\n",
      "When input is [67, 1, 54, 1] the target is: 59\n",
      "When input is [67, 1, 54, 1, 59] the target is: 62\n",
      "When input is [67, 1, 54, 1, 59, 62] the target is: 67\n",
      "When input is [67, 1, 54, 1, 59, 62, 67] the target is: 58\n",
      "When input is [67, 1, 54, 1, 59, 62, 67, 58] the target is: 1\n",
      "When input is [73] the target is: 68\n",
      "When input is [73, 68] the target is: 71\n",
      "When input is [73, 68, 71] the target is: 1\n",
      "When input is [73, 68, 71, 1] the target is: 56\n",
      "When input is [73, 68, 71, 1, 56] the target is: 54\n",
      "When input is [73, 68, 71, 1, 56, 54] the target is: 65\n",
      "When input is [73, 68, 71, 1, 56, 54, 65] the target is: 65\n",
      "When input is [73, 68, 71, 1, 56, 54, 65, 65] the target is: 58\n",
      "When input is [58] the target is: 71\n",
      "When input is [58, 71] the target is: 58\n",
      "When input is [58, 71, 58] the target is: 1\n",
      "When input is [58, 71, 58, 1] the target is: 54\n",
      "When input is [58, 71, 58, 1, 54] the target is: 1\n",
      "When input is [58, 71, 58, 1, 54, 1] the target is: 59\n",
      "When input is [58, 71, 58, 1, 54, 1, 59] the target is: 54\n",
      "When input is [58, 71, 58, 1, 54, 1, 59, 54] the target is: 65\n",
      "When input is [75] the target is: 58\n",
      "When input is [75, 58] the target is: 1\n",
      "When input is [75, 58, 1] the target is: 61\n",
      "When input is [75, 58, 1, 61] the target is: 54\n",
      "When input is [75, 58, 1, 61, 54] the target is: 73\n",
      "When input is [75, 58, 1, 61, 54, 73] the target is: 58\n",
      "When input is [75, 58, 1, 61, 54, 73, 58] the target is: 72\n",
      "When input is [75, 58, 1, 61, 54, 73, 58, 72] the target is: 1\n"
     ]
    }
   ],
   "source": [
    "# manual seed for random generator for this code if you would like to reproduce results\n",
    "#torch.manual_seed(1337)\n",
    "batch_size = 4 # how many chunks we will process at once\n",
    "chunk_size = 8 # max context length for predictions\n",
    "\n",
    "def get_batch(split):\n",
    "    #generating a small batch of data of inputs x and targets y\n",
    "    data = train_data if split== 'train' else val_data\n",
    "    ix = torch.randint(len(data) - chunk_size, (batch_size,)) # x position for random batch\n",
    "    x = torch.stack([data[i:i+chunk_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+chunk_size+1] for i in ix])\n",
    "    return x,y\n",
    "\n",
    "x_batch, y_batch = get_batch('train')\n",
    "print('inputs:')\n",
    "print(x_batch.shape)\n",
    "print(x_batch)\n",
    "print('targets:')\n",
    "print(y_batch.shape)\n",
    "print(y_batch)\n",
    "\n",
    "print('----')\n",
    "\n",
    "for b in range(batch_size): # batch dimension\n",
    "    for t in range(chunk_size): # time dimension\n",
    "        context = x_batch[b, :t+1]\n",
    "        target = y_batch[b,t]\n",
    "        print(f\"When input is {context.tolist()} the target is: {target}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Neural Network\n",
    "\n",
    "Now that the data is prepared into train/validatin sets and batching, randomized positioning has been defined, and we have encoded those batches. I will now implement a neural network with the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[69, 54, 73, 62, 58, 67, 73,  1],\n",
      "        [72, 72,  1, 68, 59,  1, 78, 68],\n",
      "        [ 1, 61, 58,  1, 73, 68, 68, 64],\n",
      "        [ 9,  1, 73, 61, 68, 74, 60, 61]])\n"
     ]
    }
   ],
   "source": [
    "print(x_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining module for a simple Bigram Language Model\n",
    "\n",
    "1. Creating token embedding tables for positional reference\n",
    "2. Creating a embedding table for multidimensional tensor for token pairs (B,T,C)\n",
    "3. Defining loss function (cross_entorpy) and making sure it aligns with expected loss (-ln(1/88)) -- 88 being the number of unique characters (vocab_size variable)\n",
    "4. Generate function will get predictions, apply the softmax activation function to find probability most likely next character, and append the character with the highest probability to the end of the running sequence.\n",
    "\n",
    "Simple model and progress is made but will need to implement context and transformsers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([256, 88])\n",
      "tensor(4.9272, grad_fn=<NllLossBackward0>)\n",
      "\n",
      "B9qYE—IPz%8\n",
      "Vbn#Hl-(V2Hh•vL; i•t—0w#3*pUn$0“KGZ 3﻿Jt8nIjs—JKEeSv0GowTlSU5’c—!wjHRvM]8.E—!(“09qvF*4ft\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "#torch.manual_seed(1337)\n",
    "\n",
    "class BigramLanguageModel(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "        # creating an embedding table for position reference to block out current positions (similar to a visited table)\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        # idx and targets are both (B,T) tensor of integers\n",
    "        logits = self.token_embedding_table(idx) # (B,T,C) token pairing for tensor\n",
    "        # due to pytorch cross entropy wanting the tensor to be (B,C,T) we will need to reshape\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            # loss definition will be cross entropy\n",
    "            loss = F.cross_entropy(logits, targets) # expecting loss of 4.477368 (-ln(1/88))\n",
    "\n",
    "        return logits, loss\n",
    "    \n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        # idx is (B,T) array of indices in current context\n",
    "        for _ in range(max_new_tokens):\n",
    "            # get predictions\n",
    "            logits, loss = self(idx)\n",
    "            #focusing on last step 'time' step\n",
    "            logits = logits[:, -1, :] # becomes (B, C)\n",
    "            # using softmax as activation function\n",
    "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
    "            # sample from distribution\n",
    "            idx_next = torch.multinomial(probs,num_samples=1) # (B, 1)\n",
    "            # appending sampled index to the running sequence\n",
    "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
    "        return idx\n",
    "    \n",
    "model = BigramLanguageModel(vocab_size)\n",
    "logits, loss = m(x_batch, y_batch)\n",
    "print(logits.shape)\n",
    "print(loss)\n",
    "\n",
    "print(decode(model.generate(idx = torch.zeros((1,1), dtype=torch.long),max_new_tokens=100)[0].tolist()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First run of model:\n",
    "\n",
    "torch.Size([32, 88])\n",
    "tensor(4.9037, grad_fn=<NllLossBackward0>)\n",
    "\n",
    "7-OW rt—60uL(EQzCcrraUtm(hTpb#GQ%Kgbq#﻿3BA9•AkGgcBP-RdK\n",
    "r“5Oyf(W?C;K/keSI\n",
    "0yPtYbJ’”8zCz#UJ(:,J5••AZR\n",
    "\n",
    "Which is expected as it is random -- training is yet to be done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimizer - using Adam and using higher learning rate because of smaller sample data\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.4078376293182373\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "\n",
    "for steps in range(10000):\n",
    "    #sample batch\n",
    "    x_batch,y_batch = get_batch('train')\n",
    "\n",
    "    #eval loss\n",
    "    logits, loss = model(x_batch,y_batch)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "print(loss.item())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " thelll  thim braidd bay id o am\n",
      " iugof  recing  thinpat Thary the a bas,”\n",
      " thivemupirid, baps,  s. \n"
     ]
    }
   ],
   "source": [
    "# Model results after optimizing -- progress but still needs work.\n",
    "print(decode(model.generate(idx = torch.zeros((1,1), dtype=torch.long),max_new_tokens=100)[0].tolist()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my_pytorch_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
